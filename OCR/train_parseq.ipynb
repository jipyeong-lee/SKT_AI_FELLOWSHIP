{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43590,"status":"ok","timestamp":1666795384261,"user":{"displayName":"이지평","userId":"04288060060884002898"},"user_tz":-540},"id":"dB3H61Bp2goo","outputId":"e7e6a06b-fb77-4d33-90d8-8358f3094b69"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2214,"status":"ok","timestamp":1666795386457,"user":{"displayName":"이지평","userId":"04288060060884002898"},"user_tz":-540},"id":"X6SnBBQNGd08","outputId":"33a88112-1e76-4aa6-f962-b6ce3697326c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/SKT_AIF/OCR/OCR/models\n"]}],"source":["cd /content/drive/MyDrive/Colab Notebooks/SKT_AIF/OCR/OCR/models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVFzWBbnbIvD"},"outputs":[],"source":["%%bash\n","cd parseq\n","pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zt7yTxbDndMg","executionInfo":{"status":"ok","timestamp":1666795967272,"user_tz":-540,"elapsed":394150,"user":{"displayName":"이지평","userId":"04288060060884002898"}},"outputId":"86c1d7d0-30ad-4054-ac25-a7d75de130dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["[2022-10-26 14:46:54,316][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpjccm4yer\n","[2022-10-26 14:46:54,318][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpjccm4yer/_remote_module_non_scriptable.py\n","  | Name       | Type           | Params\n","----------------------------------------------\n","0 | encoder    | Encoder        | 21.4 M\n","1 | decoder    | Decoder        | 2.4 M \n","2 | head       | Linear         | 939 K \n","3 | text_embed | TokenEmbedding | 938 K \n","4 | dropout    | Dropout        | 0     \n","----------------------------------------------\n","25.6 M    Trainable params\n","0         Non-trainable params\n","25.6 M    Total params\n","102.547   Total estimated model params size (MB)\n","Using 16bit native Automatic Mixed Precision (AMP)\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Loading `train_dataloader` to estimate number of stepping batches.\n","[2022-10-26 14:47:01,994][strhub.data.dataset][INFO] - dataset root:\t/content/drive/MyDrive/Colab Notebooks/SKT_AIF/OCR/OCR/models/parseq/data/train/real/Paper\n","[2022-10-26 14:47:40,220][strhub.data.dataset][INFO] - \tlmdb:\ttrain\tnum samples: 70004\n","[2022-10-26 14:48:13,024][strhub.data.dataset][INFO] - \tlmdb:\ttrain2\tnum samples: 504970\n","[2022-10-26 14:48:45,471][strhub.data.dataset][INFO] - \tlmdb:\ttrain3\tnum samples: 531912\n","[2022-10-26 14:51:56,642][strhub.data.dataset][INFO] - \tlmdb:\ttrain4\tnum samples: 725469\n","[2022-10-26 14:52:01,499][numexpr.utils][INFO] - NumExpr defaulting to 2 threads.\n","Sanity Checking: 0it [00:00, ?it/s][2022-10-26 14:52:02,006][strhub.data.dataset][INFO] - dataset root:\t/content/drive/MyDrive/Colab Notebooks/SKT_AIF/OCR/OCR/models/parseq/data/val\n","[2022-10-26 14:52:14,062][strhub.data.dataset][INFO] - \tlmdb:\ttest2\tnum samples: 104244\n","Epoch 0:   0% 0/5964 [00:00<?, ?it/s] Error executing job with overrides: []\n","Traceback (most recent call last):\n","  File \"parseq/train.py\", line 79, in main\n","    trainer.fit(model, datamodule=datamodule, ckpt_path=config.ckpt_path)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 771, in fit\n","    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\n","    return trainer_fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\n","    results = self._run(model, ckpt_path=self.ckpt_path)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\n","    return self._run_train()\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\n","    self.fit_loop.run()\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 204, in run\n","    self.advance(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 266, in advance\n","    self._outputs = self.epoch_loop.run(self._data_fetcher)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 204, in run\n","    self.advance(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 208, in advance\n","    batch_output = self.batch_loop.run(batch, batch_idx)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 204, in run\n","    self.advance(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 88, in advance\n","    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 204, in run\n","    self.advance(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 207, in advance\n","    self.optimizer_idx,\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 256, in _run_optimization\n","    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 378, in _optimizer_step\n","    using_lbfgs=is_lbfgs,\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1595, in _call_lightning_module_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py\", line 1646, in optimizer_step\n","    optimizer.step(closure=optimizer_closure)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/optimizer.py\", line 168, in step\n","    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/strategies/strategy.py\", line 193, in optimizer_step\n","    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/precision/native_amp.py\", line 85, in optimizer_step\n","    closure_result = closure()\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 148, in __call__\n","    self._result = self.closure(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 134, in closure\n","    step_output = self._step_fn()\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 427, in _training_step\n","    training_step_output = self.trainer._call_strategy_hook(\"training_step\", *step_kwargs.values())\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1765, in _call_strategy_hook\n","    output = fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/strategies/strategy.py\", line 333, in training_step\n","    return self.model.training_step(*args, **kwargs)\n","  File \"/content/drive/MyDrive/Colab Notebooks/SKT_AIF/OCR/OCR/models/parseq/strhub/models/parseq/system.py\", line 247, in training_step\n","    out = self.decode(tgt_in, memory, tgt_mask, tgt_padding_mask, tgt_query_mask=query_mask)\n","  File \"/content/drive/MyDrive/Colab Notebooks/SKT_AIF/OCR/OCR/models/parseq/strhub/models/parseq/system.py\", line 93, in decode\n","    return self.decoder(tgt_query, tgt_emb, memory, tgt_query_mask, tgt_mask, tgt_padding_mask)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/drive/MyDrive/Colab Notebooks/SKT_AIF/OCR/OCR/models/parseq/strhub/models/parseq/modules.py\", line 99, in forward\n","    update_content=not last)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/drive/MyDrive/Colab Notebooks/SKT_AIF/OCR/OCR/models/parseq/strhub/models/parseq/modules.py\", line 78, in forward\n","    query = self.forward_stream(query, query_norm, content_norm, memory, query_mask, content_key_padding_mask)[0]\n","  File \"/content/drive/MyDrive/Colab Notebooks/SKT_AIF/OCR/OCR/models/parseq/strhub/models/parseq/modules.py\", line 67, in forward_stream\n","    tgt2, ca_weights = self.cross_attn(self.norm1(tgt), memory, memory)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\", line 1160, in forward\n","    attn_mask=attn_mask, average_attn_weights=average_attn_weights)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 5179, in multi_head_attention_forward\n","    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 4858, in _scaled_dot_product_attention\n","    attn = dropout(attn, p=dropout_p)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 1252, in dropout\n","    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n","RuntimeError: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 14.76 GiB total capacity; 13.29 GiB already allocated; 49.75 MiB free; 13.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\n","Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n","Epoch 0:   0% 0/5964 [00:13<?, ?it/s]\n"]}],"source":["!python parseq/train.py\n","#ckpt_path='outputs/parseq/2022-10-25_13-56-51/checkpoints/last.ckpt'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CA87QK4vb8_l"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"welWMTSNb9B3"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}